#!/usr/bin/env python

# CS4242 Social Media Computing
# Assignment 1
# See LICENSE for details

# Classifier mode.
classifier_mode = 'local'

# Path to the training sets of tweets.
# This is only used if the classifier mode is local.
training_sets = {
    'nus1': './dataset/training/NUS1.txt',
    'nus2': './dataset/training/NUS2.txt',
    'dbs1': './dataset/training/DBS1.txt',
    'dbs2': './dataset/training/DBS2.txt',
    'starhub': './dataset/training/STARHUB.txt'
}

# Path to the training set of tweets.
# This is only used if the classifier mode is global.
training_file = './dataset/training/NUS1.txt'

# Stopword sources, in order.
stopword_sources = [
    'nltk:english',
    'file:./dataset/stopwords.smartdatacollective.com.txt'
]

# Path to a vocabulary normalization map.
vocab_map_file = './dataset/normalization_map.emnlp.txt'

# Should we classify tweets from an actual source, or from a file?
# tweet_source = "twitter"
# tweet_source = "link"
tweet_source = "file"

# If we are classifying tweets from a file, then these parameters are required.
tweet_file = "./dataset/testing/TEST.txt"

# If we are classifying tweets from the actual source, then these API
# parameters are required.
oauth_consumer_key = ""
oauth_consumer_secret = ""

oauth_token_key = ""
oauth_token_secret = ""

# vim: set ts=4 sw=4 et:
